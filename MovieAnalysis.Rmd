---
title: "Statistical Modeling: IMDB's User Movie Ratings Analysis"
author:
- Juan Borgnino (jb3852)
- Carolyn Morris (cm3491)
- Jose Ramirez (jdr2162)
- Manuel Rueda (mr3523)
date: "December 15, 2015"
abstract: |
  In this analysis we take IMDB's movie rating and descriptive data, combine it with the Academy Awards' Best Picture nominations, and identify the driving variables behind a highly rated movie. A number of linear and non-linear models are utilized for this purpose. We identify the budget being a relevant variable for movies on the Animation and Action genres, but not on the rest. For all movies, the number of votes was the best predictor, implying that more popular movies tend to be rated higher (or the other way around). The number of 'Best Picture' nominations seems to play a minor but significant role.
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=4.5, echo=FALSE, warning=FALSE, message=FALSE)
```

```{r Setup}
library("gettingtothebottom")
library("RCurl")
library("corrplot")
library("GGally")
library("reshape2")
library("plyr")
library("leaps")
```


```{r Data Processing}
data("moviebudgets")
## Remove the individual rating columns.
movies <- moviebudgets[, !names(moviebudgets) %in% c("r1","r2","r3","r4","r5","r6","r7","r8","r9","r10")]
movies$title <- as.character(movies$title)

## Oscar awards.
oscars <- read.csv("academy_awards.csv", stringsAsFactors = F)
oscars$Year2 <- as.numeric(substr(oscars$Year, 0, 4))
bestP <- oscars[oscars$Category == "Best Picture" & oscars$Year2 <= 2000,]
movies$nominated <- rep(0, nrow(movies))
movies[!is.na(match(gsub(", The", "", gsub('"', '', movies$title)), gsub("The ", "", bestP$Nominee))), "nominated"] =+ 1
```

#Data Sources

Our dataset consists on movie rating and budget data for 5,183 films, scraped from the [Internet Movie Database](http://www.imdb.com/interfaces/) FTP site, paired with a historical list of Academy Awards Best Picture Nominations, retrieved from [Agg Data](https://www.aggdata.com/awards/oscar). Both data sources are legaly made available to the general public. For our analysis, we are interested in analyzing the interactions between IMDB ratings assigned by the general public and a list of other possible explainatory variables, which are listed on the following table.

------------------------------------------------------------
Variable  Description
--------- ---------------------------------------------------
title     Title of the movie.
 
year      Year the movie was released.
 
budget    Total budget (if known) in US dollars.
 
length    Length of movie (in minutes).

rating    Average IMDb user rating.
 
votes     Number of IMDb users who rated the movie.

mpaa      MPAA rating.
      
nominated Binary variable indicating if the movie was
          nominated for the 'Best Movie'.

genre     Binary variables indicating whether movie belongs
          to any of the following genres: action, animation,
          comedy, drama, documentary, romance, short.
------------------------------------------------------------

\pagebreak

# Exploratory Data Analysis

### Summary of Data
Before begining with the formal statistical analysis, it is necessary to perform exploratory analysis on the data to identify possible patterns. We begin by looking at the summary information for each of the variables.

```{r Summary1 Statistics}
summary(movies)

genres <- c("Action", "Animation", "Comedy", "Drama", "Documentary", "Romance", "Short")
```

We see a couple of interesting things here:

(1) 25% of the movies have received less than 70 *votes*. We will remove this lower quantile, as we want to focus our attention on those films for which a larger consensus has been reached.
(2) On the *length* column (below), we can see a cluster of short films on the right hand side of the plots. They clearly have a different behavior than full-feature ones, so we will exclude them from the analysis.
(3) The *mpaa* variable has 4 categorical variables, each corresponding to a rating. We will replace this with *mpnum* (0:n/a, 1:PG, 2:PG-13, 3:NC-17, 4:R).
(4) The *budget* variable is defined on US dollars, but has not been adjusted for inflation. We will retrieve the yearly CPI data to convert this series into nominal terms.

```{r Summary2, cache = T}
ggpairs(movies[,!names(movies) %in% c(genres, "title")], 
        diag=list(continuous="density", discrete="bar"),
        axisLabels="none", params=c(alpha=1/3))
```

```{r Data Cleaning}
## Assign numerical variable to mpaa.
mpMat <- data.frame(levels(movies$mpaa), c(1,4,2,3,5))
names(mpMat) <- c("mpaa","mpnum")
movies <- merge(movies, mpMat)
movies = movies[, !names(movies) %in% c('mpaa')]

## Remove movies with less than 70 votes.
movies <- movies[movies$votes >= 70,]

## Deflate budget variable.
infl <- read.csv("infl.csv", stringsAsFactors = F)
movies$adjBudget <- as.numeric(movies$budget / infl[match(movies$year, infl$Date),2] * 100, scientific=F)
movies = movies[, !names(movies) %in% c('budget')]

## Remove short films.
movies <- movies[movies$Short == 0,]
movies <- movies[, !names(movies) %in% c('Short')]

## Remove titles column.
movies <- movies[, !names(movies) %in% c('title')]
```

After proceding with these changes, we identified one extreme outlier in terms of nominal budget: "Voyna i mir", or "War & Peace" (1966). Investigating this movie, we see that it was sponsored by the soviet party, which covered its stratosferical expenses. For the purposes of thie analysis, we will omit it from the sample.

```{r Voyna i mir, fig.width=6, fig.height=3}
#ggplot(data=movies, aes(adjBudget, votes)) + geom_point() + labs(x = "Adj. Budget", y = "Votes")
## Remove outlying "Voyna i mir".
movies <- movies[-which.max(movies$adjBudget),]
```

### Correlations Across Variables
Now we present the charts of paired variables, along with a visual representation of the correlation matrix.

```{r EDA}
## All movies.
ggpairs(movies[,!names(movies) %in% c(genres, "title")], 
        diag=list(continuous="density", discrete="bar"),
        axisLabels="none", params=c(alpha=1/3))

allCorr <- cor(movies[3:ncol(movies)])

cex.before <- par("cex")
par(cex = 0.7, cex.axis = 0.7)
corrplot(allCorr, method = "color", addCoef.col="black", order = "hclust", type="lower",
         tl.cex = 1/par("cex"), cl.cex = 1/par("cex"), tl.col="black", addCoefasPercent = TRUE)
par(cex = cex.before)
```

Here we can see a positive correlation between rating and: *votes* & *nominated*. The correlation with *adjBudget* is surprisingly low, suggesting that this is not one of the main driving factors on explaining the user preference for a film.

Other interesting observations:

* Action & Animation movies tend to receive a high budget, which makes sense considering they rely intensively on special effects.

* Dramas and Romance films tend to be nominated more frequently.

* Dramas and Comedies don't tend to go together, while Comedies and Romance are a frequent mix (Rom-Coms).

We are however mostly interested in how the IMDB rating of a movie can be predicted utilizing the rest of the variables, so we will restrict our atention to these specific interactions. An interesting excercise is to investigate how the rates correlate with the rest of the variables, particularly the most significant ones (*votes, adjBudget & nominations*) when splitting the dataset by genre, and for this we will make use of the **Test of Independence**, defined in the following way:

*Let $X$ and $Y$ have a bivariate normal distribution with means $\mu_1$ and $\mu_2$, positive variances $\sigma_1^2$ and $\sigma_2^2$, and correlation coefficient $\rho$. We wish to test the hypothesis that $X$ and $Y$ are independent.* We will use $R$, the sample estimate of $\rho$, to test the null hypothesis $H_0:\rho = 0$ against the alternative $H_1:\rho \neq 0$. For this we build the following statistic, which has a *t*-distribution with $n-2$ degrees of freedom.

\begin{center}$T = \left(\frac{R\sqrt{n-1}}{\sqrt{1-R^2}}\right)$\end{center}

We reject the null hypothesis with a level $\alpha$ if $|T| \geq t_{\alpha/2, n-2}$. This is equivalent to applying the `cor.test()` R function, however for completeness we have built our own function following the described procedure in this analysis. In our case we don't have evidence of normality in our variables, but we will assume so for practical purposes. Below are the results, per genre.

```{r Test of Independence}
## Melt the data to make the genres more readable.
molten <- melt(movies, id.vars = c("year", "length","rating", "votes", "adjBudget",
                                   "mpnum", "nominated"), variable.name = "genre")
molten <- molten[!molten$value == 0,]

## Loop to see on which genres budget, votes and rating are independent.
## This is equivalent to the cor.test function.
voteTest <- function(x) {
  r <- cor(x$rating, x$votes)
  t <- (r * sqrt(nrow(x)-2)) / (sqrt(1-r^2))
  o <- data.frame(r,t, qt(0.05/2, nrow(x)-2), nrow(x), abs(t) > abs(qt(0.05/2, nrow(x)-2)),1 - pt(t, nrow(x)-2))
  colnames(o) <- c("r", "t", "t-crit", "n", "sig?", "p-value")
  o
}

budgetTest <- function(x) {
  r <- cor(x$rating, x$adjBudget)
  t <- (r * sqrt(nrow(x)-2)) / (sqrt(1-r^2))
  o <- data.frame(r,t, qt(0.05/2, nrow(x)-2), nrow(x), abs(t) > abs(qt(0.05/2, nrow(x)-2)), 1 - pt(t, nrow(x)-2))
  colnames(o) <- c("r", "t", "t-crit", "n", "sig?", "p-value")
  o
}

oscarTest <- function(x) {
  r <- cor(x$rating, x$nominated)
  t <- (r * sqrt(nrow(x)-2)) / (sqrt(1-r^2))
  o <- data.frame(r,t, qt(0.05/2, nrow(x)-2), nrow(x), abs(t) > abs(qt(0.05/2, nrow(x)-2)), 1 - pt(t, nrow(x)-2))
  colnames(o) <- c("r", "t", "t-crit", "n", "sig?", "p-value")
  o
}

cVotes <- ddply(.data=molten, .variables=.(genre), .fun=voteTest)
print("rating vs. votes")
cVotes

cBudget <- ddply(.data=molten, .variables=.(genre), .fun=budgetTest)
print("rating vs. budget")
cBudget

cOscar <- ddply(.data=molten, .variables=.(genre), .fun=oscarTest)
print("rating vs. oscar")
cOscar
```

The main observations from this analysis:

* The number of votes are highly correlated with the movie rating in all cases, except documentaries, where we have few observations.

* Action and Animation movies tend to perform better in terms of rating when they have a higher rating. This is in line with our previous observations.

* Movies that have been nominated for Best Picture also tend to receive better ratings.

### Correlations Across Variables
We have seen that ratings across different genres exhibit different correlations, but it would be good to have a more formal proof 


# Modeling

### Best Subset Selection
```{r}
#Create Training set
set.seed(1)
train.index = sample(dim(movies)[1], round(dim(movies)[1]*0.8))
test.index = c(1:dim(movies)[1])[-c(train.index)]
train.set = movies[train.index, ]
test.set = movies[test.index, ]

mse.test.errors = vector()
inter.column=rep(1,dim(test.set)[1])

best.subset = regsubsets(rating ~., train.set, nvmax = 12)

```

We plot each model obtained using best subset selection againts the test MSE to obtain the best model of all.

```{r}
for(i in 1:(dim(train.set)[2] - 1)){
  coef.best = coef(best.subset, id = i)
  predic = as.matrix(cbind(inter.column,test.set[ , names(test.set) %in% c(names(coef.best))])) %*% coef.best[] 
  errors = mean((test.set[,"rating"] - predic)^2)
  mse.test.errors = cbind(mse.test.errors, errors)
}

qplot(x = c(1:(dim(train.set)[2] - 1)), y = as.numeric(mse.test.errors), main = 'Test MSE vs Number of Predictors', xlab = 'Nr. of Predictors',
     ylab = 'Test MSE', geom = c("point", "path"))
```

This is the best model obtained. It includes variables year, length, votes, Action, Animation, Comedy,
Dramam, Documentary and adjBudget. As we can observe, all predictors are related to the response and we are obtaining an adjusted R squared value of 0.402, meaning that our model is explaining 40% of the variance of the rating, and a Test MSE of 1.092. However, plotting the residuals we find evidence of a non linear relationship between the predictors and the response. 

```{r}
lm.fit = lm(rating ~ year + length + votes + Action + Animation + Comedy
            + Drama + Documentary + adjBudget, data = train.set)
par(mfrow = c(2,2))
min(mse.test.errors)
summary(lm.fit)
plot(lm.fit)
```

We try taking the logarithm of the variables votes. We observe that there is no discernable pattern in the distribution of the errors. 

```{r}
lm.fit.log = lm(rating ~ year + length + I(log(votes)) + Action + Animation + Comedy
            + Drama + Documentary + adjBudget, data = train.set)
plot(lm.fit.log)
```

Finally, we also observe an important improvements in terms of Adjusted R squared, since it increased aprox 8%, and the Test MSE error I now 0.98.

```{r}
summary(lm.fit.log)
MSE.test = mean((test.set$rating - fit.log.predic)^2)
MSE.test
```

### Correlation Matrix

In our analysis, we explored four different methods for modeling the movie ratings: linear regression, polynomial regression, splines, and Generalized Additive Models (GAMs).

### Linear Regression

```{r}
# Using all variables as predictors
#lm.fit <- lm(rating ~., data = movies)
#summary(lm.fit)
# This uses titles, takes forever

# Remove titles
movies <- movies[-2]
lm.fit <- lm(rating ~., data = movies)
summary(lm.fit)
```

```{r}
# Using only votes
lm.fit <- lm(rating ~ votes, data = movies)
summary(lm.fit)

plot(votes, rating, pch = 20)
abline(lm.fit, lwd = 3, col = 'red')

# Consider only action and animation movies

```

### Polynomial Regression

We extend the linear regression model by replacing it with a polynomial function. We determine which degree polynomial to use with the Analysis of Variance (ANOVA). ANOVA uses hypothesis tests to select the degree of polynomial. The null hypothesis is that model M1 is sufficient to explain the data; the alternative hypothesis is that the more complex model M2 is required. Note that M1 and M2 must be nested models, i.e. M1 is a subset of M2. If the p-value is greater than 0.05, then there is not enough evidence to reject the null, and we conclude that the simpler model, M1, is sufficient to explain the data.

```{r}
fit.1 = lm(rating ~ year + length + Action + Animation + Comedy
           + Drama + Documentary + adjBudget + log.votes, data = movies)
fit.2 = lm(rating ~ year + length + Action + Animation + Comedy
           + Drama + Documentary + adjBudget + poly(log.votes, 2), data = movies)
fit.3 = lm(rating ~ year + length + Action + Animation + Comedy
           + Drama + Documentary + adjBudget + poly(log.votes, 3), data = movies)
fit.4 = lm(rating ~ year + length + Action + Animation + Comedy
           + Drama + Documentary + adjBudget + poly(log.votes, 4), data = movies)
fit.5 = lm(rating ~ year + length + Action + Animation + Comedy
           + Drama + Documentary + adjBudget + poly(log.votes, 5), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```


We use the model obtained to predict on our test set. We obtain a Test MSE of 0.936, which is an improvement from the model without the polynomial of degree 3.

```{r}
train.fit.3 = lm(rating ~ year + length + Action + Animation + Comedy
           + Drama + Documentary + adjBudget + poly(log(votes), 3), data = train.set)

test.fit.3 = predict(train.fit.3, newdata = test.set[, !names(test.set) %in% c('rating')])
MSE.test.fit.3 = mean((test.set$rating - test.fit.3)^2)
MSE.test.fit.3

```

The variable votes seems to be a very important variable to explain the response. We regress ratings onto a polynomial of degree 3 of votes and observe that the adjusted R square is of almost 20%. Hence, we consider interesting to visualize how does this curve fit the response on a graph.

```{r}
lm.log.votes.3 = lm(rating ~ poly(log.votes,3), data = movies)
summary(lm.log.votes.3)

```

The first graph is produced by fitting a polynomial of degree 3, while the graph on the rigth is produced fitting a regular line. It is very clear, how the polynomial of degree 3 allows us to fit a more flexible curve that fits the data better.

```{r}
par(mfrow = c(1,2))
poly.fit = lm(rating ~ poly(log.votes, 3), data = movies)
lm.votes.fit = lm(rating ~ log.votes, data = movies)
vote.lims = range(movies$log.votes)
vote.grid = seq(from=vote.lims [1], to=vote.lims[2]) 

predict.linear = predict(lm.votes.fit, newdata = list(log.votes = vote.grid), se = T)
lin.bands = cbind(predict.linear$fit + 2*predict.linear$se,  predict.linear$fit - 2*predict.linear$se)
predict.poly = predict(poly.fit, newdata = list(log.votes = vote.grid), se = T)
se.bands = cbind(predict.poly$fit + 2*predict.poly$se,  predict.poly$fit - 2*predict.poly$se)

plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Polynomial of d = 3')
lines(vote.grid, predict.poly$fit, lwd = 2, col = 'darkblue')
matlines(vote.grid, se.bands, lwd = 2, col = 'red')
plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Linear Regression')
lines(vote.grid, predict.linear$fit, lwd = 2, col = 'darkblue')
matlines(vote.grid, lin.bands, lwd = 2, col = 'red')

```

We compute the test error using the polynomial of degree three only using votes as predictor.

```{r}
poly.train = lm(rating ~ poly(log(votes),3), data = train.set)
poly.test = predict(poly.train, newdata = list(votes = log(test.set$votes)))
test.error = mean((poly.test - test.set$rating)^2)
test.error
```

Next we fit a cubic spline to the data. We start by using cross-validation to obtain the degrees of freedom which minimize the test error. We find that 7 degrees of freedom provide us the lowest estimated test error.

Next we fit a cubic spline to the data. We start by using cross-validation to obtain the degrees of freedom which minimize the test error. We find that 7 degrees of freedom provide us the lowest estimated test error.
```{r}
library(boot)
cross.spline = rep(NA, 12)
for (i in 5:16) {
  fiting = glm(rating ~ bs(log.votes, df = i), data = movies)
  cross.spline[i] = cv.glm(movies, fiting, K = 10)$delta[2]
}
spline.best.df = which.min(cross.spline)
```

Points were we will fit the 4 knots.

```{r}
attr(bs(log(movies$votes) ,df = spline.best.df) ,"knots") 
```

Next we fit a spline to the data
```{r}
library(splines)
spline.fit = lm(rating ~ bs(log.votes, df = spline.best.df), data = movies)
spline.predict = predict(spline.fit, newdata = list(log.votes = vote.grid))
summary(spline.fit)

plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Cubic Sline with 4 Knots')
lines(vote.grid, spline.predict, col = 'darkgreen', lwd = 2)
abline(v = c(attr(bs(log(movies$votes) ,df=7) ,"knots") ), col = 'darkorange', lty = 2, lwd = 2)
plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Polynomial of d = 3')
lines(vote.grid, predict.poly$fit, lwd = 2, col = 'darkblue')

```

We obtain a very high test error using cubic splines.
```{r}
spline.train = lm(rating ~ bs(log(votes), df = 6), data = train.set)
spline.test = predict(spline.train, newdata = list(votes = log(test.set$votes)))
spline.error = mean((spline.test - test.set$rating)^2)
spline.error
```


```{r}
cross.spline = rep(NA, 12)
for (i in 5:16) {
  fiting = glm(rating ~ ns(log(votes), df = i), data = movies)
  cross.spline[i] = cv.glm(movies, fiting, K = 10)$delta[2]
}
ns.best.df = which.min(cross.spline)
```


```{r}
ns.fit = lm(rating ~ ns(log(votes), df = ns.best.df), data = movies)
ns.predict = predict(ns.fit, newdata = list(votes = list(log.votes = vote.grid)))
summary(ns.fit)

plot(log(movies$votes), movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Votes vs Rating')
lines(seq(min(movies$votes), max(movies$votes)), ns.predict, col = 'darkgreen', lwd = 2)
abline(v = c(attr(ns(log(movies$votes) ,df=7) ,"knots")) , col = 'darkorange', lty = 2, lwd = 2)
```


Next we fit a spline to the data

```{r}
library(splines)
spline.fit = lm(rating ~ bs(log.votes, df = spline.best.df), data = movies)
spline.predict = predict(spline.fit, newdata = list(log.votes = vote.grid))
summary(spline.fit)

plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Cubic Sline with 4 Knots')
lines(vote.grid, spline.predict, col = 'darkgreen', lwd = 2)
abline(v = c(attr(bs(log(movies$votes) ,df=7) ,"knots") ), col = 'darkorange', lty = 2, lwd = 2)
plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Polynomial of d = 3')
lines(vote.grid, predict.poly$fit, lwd = 2, col = 'darkblue')

```

We obtain a very high test error using cubic splines.

```{r}
spline.train = lm(rating ~ bs(log(votes), df = 6), data = train.set)
spline.test = predict(spline.train, newdata = list(votes = log(test.set$votes)))
spline.error = mean((spline.test - test.set$rating)^2)
spline.error
```

```{r}
cross.spline = rep(NA, 12)
for (i in 5:16) {
  fiting = glm(rating ~ ns(log(votes), df = i), data = movies)
  cross.spline[i] = cv.glm(movies, fiting, K = 10)$delta[2]
}
ns.best.df = which.min(cross.spline)
```


```{r}
ns.fit = lm(rating ~ ns(log(votes), df = ns.best.df), data = movies)
ns.predict = predict(ns.fit, newdata = list(votes = list(log.votes = vote.grid)))
summary(ns.fit)

plot(log(movies$votes), movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Votes vs Rating')
lines(seq(min(movies$votes), max(movies$votes)), ns.predict, col = 'darkgreen', lwd = 2)
abline(v = c(attr(ns(log(movies$votes) ,df=7) ,"knots")) , col = 'darkorange', lty = 2, lwd = 2)
```

The best model using log(votes) is cubic.

```{r}
summary(fit.3)
#predict(fit.3, interval = 'confidence')
```
R-squared: 0.19 --> explains 19% of the data

Log Budget
```{r}
fit.1 = lm(rating ~ log(adjBudget), data = movies)
fit.2 = lm(rating ~ poly(log(adjBudget), 2), data = movies)
fit.3 = lm(rating ~ poly(log(adjBudget), 3), data = movies)
fit.4 = lm(rating ~ poly(log(adjBudget), 4), data = movies)
fit.5 = lm(rating ~ poly(log(adjBudget), 5), data = movies)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
```
The best model using log(adjBudget) is cubic.

```{r}
summary(fit.3)
```

R-squared: 0.02 --> explains 2% of the data

### GAMs
We will now explore General Additive Models (GAMs). These extend the multiple linear regression model by allowing non-linear functions of each variable. We calculate a different function for each variable, and add together all of their contributions. This results in potentially more accurate predictions for our response variable, the movie rating. Furthermore, since the model is additive, we can interpret the effects of each variable on the movie rating.    

We now fit a GAM to predict wage using different combinations of smoothing spline functions of votes, length, and budget.
```{r}
library(gam)
fit.1 = gam(rating ~ votes + length + adjBudget, data = movies)
fit.2 = gam(rating ~ s(votes, df = 4) + length + adjBudget, data = movies)
fit.3 = gam(rating ~ s(votes, df = 4) + s(length, df = 4) + adjBudget, data = movies)
fit.4 = gam(rating ~ s(votes, df = 4) + s(length, df = 4) + s(adjBudget, df = 4), data = movies)

anova(fit.1, fit.2, fit.3, fit.4)
```
Based on the restuls of this ANOVA, there is evidence that the GAM with a smoothing spline of each variable (votes, length, adjBudget), is the best model.

```{r}
par(mfrow = c(1,3))
plot(fit.4, se = T, col = 'red')
summary(fit.4)
```

We will now use the GAM to make predictions, and determine the test MSE.

```{r}
gam.train <- gam(rating ~ s(votes, df = 4) + s(length, df = 4) + s(adjBudget, df = 4), data = train.set)
preds <- predict(gam.train, data = test.set)
```

```{r}
fit.1 = gam(rating ~ year + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.2 = gam(rating ~ s(year, df = 4) + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.3 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.4 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(votes, df = 4) + Animation + Comedy + Drama + adjBudget, data = movies)
fit.5 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(votes, df = 4) + Animation + Comedy + Drama + s(adjBudget, df = 4), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```
Based on the ANOVA results, there is evidence that the GAM with a spline for each of year, length, and budget, along with linear variables animation, comedy, and drama, is the best model.

```{r}
summary(fit.5)
```

We now make predictions on the training set using the best GAM.
```{r}
gam.train = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(votes, df = 4) + Animation + Comedy + Drama + s(adjBudget, df = 4), data = train.set)
gam.test = predict(gam.train, newdata = test.set)
gam.error = mean((gam.test - test.set$rating)^2)
gam.error
```
The test MSE is 0.944.

We repeat the GAM process, this time using natural splines instead of smoothing splines.
```{r}
fit.1 = gam(rating ~ year + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.2 = gam(rating ~ ns(year, df = 4) + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.3 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.4 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(votes, df = 4) + Animation + Comedy + Drama + adjBudget, data = movies)
fit.5 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(votes, df = 4) + Animation + Comedy + Drama + ns(adjBudget, df = 4), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

```{r}
gam.train = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(votes, df = 4) + Animation + Comedy + Drama + ns(adjBudget, df = 4), data = train.set)
gam.test = predict(gam.train, newdata = test.set)
gam.error = mean((gam.test - test.set$rating)^2)
gam.error
```
The natural splines GAM yields a Test MSE of 0.974.


# Conclusion

## Which model was the best?

------------------------------------------------------------
Model                     Test MSE
------------------------- ----------------------------------
Linear Regression           0.981
- using subset selection
- with log(votes)

Polynomial Regression       8.334
- with votes       

Polynomial Regresssion      0.937
- using subset selection
 
Spline                      4.449

GAM                         0.944
- smoothing splines         

GAM                         0.974
- natural splines
------------------------------------------------------------
