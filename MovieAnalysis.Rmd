---
title: "IMDB's User Movie Ratings Analysis"
author:
- Juan Borgnino (jb3852)
- Carolyn Morris (cm3491)
- Jose Ramirez (jdr2162)
- Manuel Rueda (mr3523)
date: "December 15, 2015"
abstract: |
  In this analysis we take IMDB's movie rating and descriptive data, combine it with the Academy Awards' Best Picture nominations, and identify the driving variables behind a highly rated movie. A number of linear and non-linear models are utilized for this purpose. We identify the budget being a relevant variable for movies on the Animation and Action genres, but not on the rest. For all movies, the number of votes was the best predictor, implying that more popular movies tend to be rated higher (or the other way around). The number of 'Best Picture' nominations seems to play a minor but significant role.
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=4.5, echo=FALSE, warning=FALSE, message=FALSE)
```

```{r Setup}
library("gettingtothebottom")
library("corrplot")
library("GGally")
library("ggfortify")
library("reshape2")
library("plyr")
library("leaps")
library("BSDA")
library("boot")
library("splines")
```


```{r Data Processing}
data("moviebudgets")
## Remove the individual rating columns.
movies <- moviebudgets[, !names(moviebudgets) %in% c("r1","r2","r3","r4","r5","r6","r7","r8","r9","r10")]
movies$title <- as.character(movies$title)

## Oscar awards.
oscars <- read.csv("academy_awards.csv", stringsAsFactors = F)
oscars$Year2 <- as.numeric(substr(oscars$Year, 0, 4))
bestP <- oscars[oscars$Category == "Best Picture" & oscars$Year2 <= 2000,]
movies$nominated <- rep(0, nrow(movies))
movies[!is.na(match(gsub(", The", "", gsub('"', '', movies$title)), gsub("The ", "", bestP$Nominee))), "nominated"] =+ 1
```

#Data Sources

Our dataset consists on movie rating and budget data for 5,183 films, scraped from the [Internet Movie Database](http://www.imdb.com/interfaces/) FTP site, paired with a historical list of Academy Awards Best Picture Nominations, retrieved from [Agg Data](https://www.aggdata.com/awards/oscar). Both data sources are legaly made available to the general public. For our analysis, we are interested in analyzing the interactions between IMDB ratings assigned by the general public and a list of other possible explainatory variables, which are listed on the following table.

------------------------------------------------------------
Variable  Description
--------- ---------------------------------------------------
title     Title of the movie.
 
year      Year the movie was released.
 
budget    Total budget (if known) in US dollars.
 
length    Length of movie (in minutes).

rating    Average IMDb user rating.
 
votes     Number of IMDb users who rated the movie.

mpaa      MPAA rating.
      
nominated Binary variable indicating if the movie was
          nominated for the 'Best Movie'.

genre     Binary variables indicating whether movie belongs
          to any of the following genres: action, animation,
          comedy, drama, documentary, romance, short.
------------------------------------------------------------

\pagebreak

# Exploratory Data Analysis

### Summary of Data
Before beginning with the formal statistical analysis, it is necessary to perform exploratory analysis on the data to identify possible patterns. We begin by looking at the summary information for each of the variables.

```{r Summary1 Statistics}
summary(movies)

genres <- c("Action", "Animation", "Comedy", "Drama", "Documentary", "Romance", "Short")
```

We see a couple of interesting things here:

(1) 25% of the movies have received less than 70 *votes*. We will remove this lower quantile, as we want to focus our attention on those films for which a larger consensus has been reached.
(2) In the *length* column (below), we can see a cluster of short films on the right hand side of the plots. They clearly have a different behavior than full-feature ones, so we will exclude them from the analysis.
(3) The *mpaa* variable has 4 categorical variables, each corresponding to a rating. We will replace this with *mpnum* (0:N/A, 1:PG, 2:PG-13, 3:NC-17, 4:R).
(4) The *budget* variable is defined on US dollars, but has not been adjusted for inflation. We will retrieve the yearly Consumer Price Index (CPI) data to convert this series into real terms.

```{r Summary2, cache = T}
ggpairs(movies[,!names(movies) %in% c(genres, "title")], 
        diag=list(continuous="density", discrete="bar"),
        axisLabels="none", params=c(alpha=1/3))
```

```{r Data Cleaning}
## Assign numerical variable to mpaa.
mpMat <- data.frame(levels(movies$mpaa), c(1,4,2,3,5))
names(mpMat) <- c("mpaa","mpnum")
movies <- merge(movies, mpMat)
movies = movies[, !names(movies) %in% c('mpaa')]

## Remove movies with less than 70 votes.
movies <- movies[movies$votes >= 70,]

## Deflate budget variable.
infl <- read.csv("infl.csv", stringsAsFactors = F)
movies$adjBudget <- as.numeric(movies$budget / infl[match(movies$year, infl$Date),2] * 100, scientific=F)
movies = movies[, !names(movies) %in% c('budget')]

## Remove short films.
movies <- movies[movies$Short == 0,]
movies <- movies[, !names(movies) %in% c('Short')]

## Remove titles column.
movies <- movies[, !names(movies) %in% c('title')]
```

After proceeding with these changes, we identified one extreme outlier in terms of real budget: "Voyna i mir", or "War & Peace" (1966). Investigating this movie, we see that it was sponsored by the Soviet party, which covered its astronomical expenses. For the purposes of thie analysis, we will omit it from the sample.

```{r Voyna i mir, fig.width=6, fig.height=3}
ggplot(data=movies, aes(adjBudget, votes)) + geom_point() + labs(x = "Adj. Budget", y = "Votes")
## Remove outlying "Voyna i mir".
movies <- movies[-which.max(movies$adjBudget),]
```

### Correlations Across Variables
Now we present the charts of paired variables, along with a visual representation of the correlation matrix.

```{r EDA}
## All movies.
ggpairs(movies[,!names(movies) %in% c(genres, "title")], 
        diag=list(continuous="density", discrete="bar"),
        axisLabels="none", params=c(alpha=1/3))

allCorr <- cor(movies[3:ncol(movies)])

cex.before <- par("cex")
par(cex = 0.7, cex.axis = 0.7)
corrplot(allCorr, method = "color", addCoef.col="black", order = "hclust", type="lower",
         tl.cex = 1/par("cex"), cl.cex = 1/par("cex"), tl.col="black", addCoefasPercent = TRUE)
par(cex = cex.before)
```

Here we can see a positive correlation between rating and *votes*, as well as rating and *nominated*. The correlation with *adjBudget* is surprisingly low, suggesting that this is not one of the main driving factors on explaining the user preference for a film.

Other interesting observations:

* Action & Animation movies tend to receive a high budget, which makes sense considering they rely intensively on special effects.

* Dramas and Romance films tend to be nominated more frequently.

* Dramas and Comedies don't tend to go together, while Comedies and Romance are a frequent mix (Rom-Coms).

We are however mostly interested in how the IMDB rating of a movie can be predicted utilizing the rest of the variables, so we will restrict our attention to these specific interactions. An interesting excercise is to investigate how the ratings correlate with the rest of the variables, particularly the most significant ones (*votes, adjBudget, & nominations*) when splitting the dataset by genre, and for this we will make use of the **Test of Independence**, which is defined as follows:

*Let $X$ and $Y$ have a bivariate normal distribution with means $\mu_1$ and $\mu_2$, positive variances $\sigma_1^2$ and $\sigma_2^2$, and correlation coefficient $\rho$. We wish to test the hypothesis that $X$ and $Y$ are independent.* We will use $R$, the sample estimate of $\rho$, to test the null hypothesis $H_0:\rho = 0$ against the alternative $H_1:\rho \neq 0$. For this we build the following statistic, which has a *t*-distribution with $n-2$ degrees of freedom.

\begin{center}$T = \left(\frac{R\sqrt{n-1}}{\sqrt{1-R^2}}\right)$\end{center}

We reject the null hypothesis with a level $\alpha$ if $|T| \geq t_{\alpha/2, n-2}$. This is equivalent to applying the `cor.test()` R function, however for completeness we have built our own function that will lead us to the same results.[^1] In our case we don't have evidence of normality in our variables, but we will assume so for practical purposes. Below are the results, per genre.

```{r Test of Independence}
## Melt the data to make the genres more readable.
molten <- melt(movies, id.vars = c("year", "length","rating", "votes", "adjBudget",
                                   "mpnum", "nominated"), variable.name = "genre")
molten <- molten[!molten$value == 0,]

## Loop to see on which genres budget, votes and rating are independent.
## This is equivalent to the cor.test function.
voteTest <- function(x) {
  r <- cor(x$rating, x$votes)
  t <- (r * sqrt(nrow(x)-2)) / (sqrt(1-r^2))
  o <- data.frame(r,t, qt(0.05/2, nrow(x)-2), nrow(x), abs(t) > abs(qt(0.05/2, nrow(x)-2)),1 - pt(t, nrow(x)-2))
  colnames(o) <- c("r", "t", "t-crit", "n", "sig?", "p-value")
  o
}

budgetTest <- function(x) {
  r <- cor(x$rating, x$adjBudget)
  t <- (r * sqrt(nrow(x)-2)) / (sqrt(1-r^2))
  o <- data.frame(r,t, qt(0.05/2, nrow(x)-2), nrow(x), abs(t) > abs(qt(0.05/2, nrow(x)-2)), 1 - pt(t, nrow(x)-2))
  colnames(o) <- c("r", "t", "t-crit", "n", "sig?", "p-value")
  o
}

oscarTest <- function(x) {
  r <- cor(x$rating, x$nominated)
  t <- (r * sqrt(nrow(x)-2)) / (sqrt(1-r^2))
  o <- data.frame(r,t, qt(0.05/2, nrow(x)-2), nrow(x), abs(t) > abs(qt(0.05/2, nrow(x)-2)), 1 - pt(t, nrow(x)-2))
  colnames(o) <- c("r", "t", "t-crit", "n", "sig?", "p-value")
  o
}

cVotes <- ddply(.data=molten, .variables=.(genre), .fun=voteTest)
print("rating vs. votes")
cVotes

cBudget <- ddply(.data=molten, .variables=.(genre), .fun=budgetTest)
print("rating vs. budget")
cBudget

cOscar <- ddply(.data=molten, .variables=.(genre), .fun=oscarTest)
print("rating vs. oscar")
cOscar
```

The main observations from this analysis:

* The number of votes are highly correlated with the movie rating in all cases, except documentaries, where we have few observations.

* Action and Animation movies tend to perform better in terms of rating when they have a higher budget. This is in line with our previous observations.

* Movies that have been nominated for Best Picture also tend to receive better ratings. `NA`'s appear because some genres have not received this nomination at all.

### Distribution of Ratings
We have seen that ratings across different genres exhibit different correlations, and now we wish to investigate with more detail the distribution of votes among these different categories. First, we will comment on the median of this variable; can we say that it is below the centered-rating of 5 starts? A boxplot of the data and the **Sign-Test** can help us investigate this.

For the Sign-Test, we will test the following hypothesis:

\begin{center}$H_0:\theta < 5$ vs. $H_1:\theta \geq 5$\end{center}

The test can be performed via the `SIGN.test` function in R, but here we have decided to implement the function step by step.[^1]

```{r Sign-Test}
# Sign-Test with Small Sample.
med <- 5
set.seed(1)
subset=sample(dim(movies)[1] ,size=30)
y<-movies$rating[subset]
print("Ratings subset:")
print(z <- sort(y))
print("Length of subset:")
print(n <- length(z))
print("Observations greater than 5:")
print(b <- sum(z > med))
print("p-value:")
p_value=1-pbinom(b-1,n,.5)
print(p_value)

print("sample median:")
trueMed <- median(y)
trueMed
# If p-value is less than 0.05 then H0 is rejected. The median is greater than 5.
# SIGN.test(y, md = 5, alternative = "greater", conf.level = 0.95)
```

With this small *p-value*, we reject the null hypothesis of the median being less than 5 stars. Combining this information with a box plot:

```{r Box Plot}
ggplot(molten, aes(x = genre, y = rating)) + geom_boxplot() +
  labs(x = "Genre", y = "Rating") + geom_hline(aes(yintercept=trueMed, color="navy"))
```

The graphical representation also confirms what the sign-test suggested. We see that votes tend to be concentrated on the upper part of the spectrum, with a median around 6.5. Most movies tend to be "good" ones, or they tend to be rated more. Now, we see some difference on the distributions across genres. We now wish to test if this difference is statistically significant. For this we will use the **Mann–Whitney–Wilcoxon** statistic on a couple of pairs, which will test for:

\begin{center}$P(Y \leq y) = P(X + \Delta \leq y) = F(y - \Delta)$\end{center}

\begin{center}$H_0:\Delta = 0$ vs. $H_1:\Delta \geq 0$\end{center}

Under $H_0$, the distributions of $X$ and $Y$ are the same, and we can combine the samples to have one large sample of $n = n1 + n2$ observations. We will use the `wilcox.test()` command in R to perform this test. Below: statistic & P-Value.

```{r}
print("Drama & Comedy")
w <- wilcox.test(rating ~ genre, data=molten[molten$genre %in% c("Drama","Comedy"),])
cbind(w$statistic, w$p.value)
print("Comedy & Romance")
w <- wilcox.test(rating ~ genre, data=molten[molten$genre %in% c("Comedy","Romance"),])
cbind(w$statistic, w$p.value)
print("Action & Comedy")
w <- wilcox.test(rating ~ genre, data=molten[molten$genre %in% c("Action","Comedy"),])
cbind(w$statistic, w$p.value)
```

First, looking at Drama and Comedy, we see that indeed their distributions are statistically different, which was also suggested by the box plots, and we can relate back to the negative correlation observed between the variables (i.e. few Dramatic Comedies exist). Romance and Comedy however do tend to go together, but still the WHM test shows us that their distributions are different. Even for Action and Comedy, two genres on which the box plot suggest a similar distribution, we do not find the null hypothesis significant at $\alpha=0.01$.

Given this, it is convenient to keep a distintion between movies of different categories. There are two ways to achieve this:

* Subset the data into different groups, one per genre.

* Keep binary variables to identify when a given film belongs to a specific genre.

We have decided to take the second approach, since this way we don't reduce the sample size of categories such as Documentaries, and we can also account for movies on which more than one genre is present (i.e. Action + Animation, Romance + Comedy, etc.).

# Statistical Modeling of Ratings

## Best Subset Selection

Having completed the exploratory analysis of the data, we proceed to perform a number of statistical modeling techniques. It is natural to start with **Best Subset Selection**, method which will help us identify which combination of variables yields an optimal Test MSE for predicting the ratings.

```{r Best Subset Selection}
#Create Training set
movies = data.frame(movies, log.votes = log(movies$votes))

set.seed(1)
train.index = sample(dim(movies)[1], round(dim(movies)[1]*0.8))
test.index = c(1:dim(movies)[1])[-c(train.index)]
train.set = movies[train.index, ]
test.set = movies[test.index, ]

mse.test.errors = vector()
inter.column=rep(1,dim(test.set)[1])

best.subset = regsubsets(rating ~., train.set[, !names(train.set) %in% c('log.votes')], nvmax = 12)
```

We plot each model obtained using best subset selection againts the test MSE to obtain the best model of all.

```{r Test MSE, fig.height=4}
for(i in 1:(dim(train.set)[2] - 2)){
  coef.best = coef(best.subset, id = i)
  predic = as.matrix(cbind(inter.column,test.set[ , names(test.set) %in% c(names(coef.best))])) %*% coef.best[] 
  errors = mean((test.set[,"rating"] - predic)^2)
  mse.test.errors = cbind(mse.test.errors, errors)
}

qplot(x = c(1:(dim(train.set)[2] - 2)), y = as.numeric(mse.test.errors), main = 'Test MSE vs Number of Predictors', xlab = 'No. of Predictors',
     ylab = 'Test MSE', geom = c("point", "path"))
```

We find that the best model only includes 7 variables, as including extra variables only increases the error. Next, we present the coefficients of this model:

```{r BSS Coeffs}
nr.covariates = which.min(mse.test.errors)
coef(best.subset, nr.covariates)
```

This is the best model obtained; it includes the variables $year$, $length$, $votes$, $Animation$, $Comedy$, $Drama$, and $adjBudget$. Somewhat surprisingly $nominated$ was not found to be relevant, which could be explained by the fact that only a small fraction of the movies (5.66%) actually received a nomination.

As we observe below, all of the selected predictors are related to the response, and we are obtaining an adjusted R squared value of 0.394, meaning that our model is explaining aprox. 40% of the variance of the rating, and a Test MSE of 1.089. However, plotting the residuals we find evidence of a non-linear relationship between the predictors and the response. 

```{r BSS Diagnostics}
lm.fit = lm(rating ~ year + length + votes + Animation + Comedy
            + Drama + adjBudget, data = train.set)
par(mfrow = c(2,2))
summary(lm.fit)
print("MSE Test Error:")
min(mse.test.errors)
autoplot(lm.fit, alpha = 1/3, smooth.colour = 'black', smooth.linetype = 'dashed', label.size = 2)
```

We take the logarithm of the variables *votes*. Now observe that there is no discernable pattern in the distribution of the errors. 

```{r Log-Linear Model}
lm.fit.log = lm(rating ~ year + length + I(log(votes)) + Animation + Comedy
            + Drama + adjBudget, data = train.set)
par(mfrow = c(2,2))
autoplot(lm.fit.log, alpha = 1/3, smooth.colour = 'black', smooth.linetype = 'dashed', label.size = 2)
```

With the logarithmic transformation of *votes*, we also observe an important improvements in terms of Adjusted R squared, since it increased approximately 8%, and the Test MSE error has decreased slightly to 1.037.

```{r Log Linear Model MSE}
summary(lm.fit.log)
fit.log.predict = predict(lm.fit.log, newdata = test.set[, !names(test.set) %in% ('rating')])
MSE.test = mean((test.set$rating - fit.log.predict)^2)
print("MSE Test Error:")
MSE.test
```

## Polynomial Regression

We extend the linear regression model by replacing it with a polynomial function. We determine which degree polynomial to use with the Analysis of Variance (ANOVA). ANOVA uses hypothesis tests to select it. The null hypothesis is that model M1 is sufficient to explain the data; the alternative hypothesis is that the more complex model M2 is required. Note that M1 and M2 must be nested models, i.e. M1 is a subset of M2. If the *p-value* is greater than 0.05, then there is not enough evidence to reject the null, and we conclude that the simpler model (lower-degree polynomial), M1, is sufficient to explain the data.

```{r Polynomial Regression}
fit.1 = lm(rating ~ year + length + Animation + Comedy
            + Drama + adjBudget  + log.votes, data = movies)
fit.2 = lm(rating ~ year + length + Animation + Comedy
            + Drama + adjBudget  + poly(log.votes,2), data = movies)
fit.3 = lm(rating ~ year + length + Animation + Comedy
            + Drama + adjBudget  + poly(log.votes,3), data = movies)
fit.4 = lm(rating ~ year + length + Animation + Comedy
            + Drama + adjBudget  + poly(log.votes,4), data = movies)
fit.5 = lm(rating ~ year + length + Animation + Comedy
            + Drama + adjBudget  + poly(log.votes,5), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

Now we use this model obtained to predict on our test set. We obtain a Test MSE of 0.991, which is an improvement from the model without the polynomial of degree 3.

```{r Poly D3}
train.fit.3 = lm(rating ~ year + length + Animation + Comedy
            + Drama + adjBudget  + poly(log.votes,3), data = train.set)

test.fit.3 = predict(train.fit.3, newdata = test.set[, !names(test.set) %in% c('rating')])
MSE.test.fit.3 = mean((test.set$rating - test.fit.3)^2)
print("MSE Test Error:")
MSE.test.fit.3

```

The variable *votes* seems to be a very important predictor for the response. We regress rating onto a polynomial of degree 3 and observe that the adjusted *R square* is aprox 19%. Hence, we consider interesting to visualize how this curve fits the response on a graph.

```{r Votes P3}
lm.log.votes.3 = lm(rating ~ poly(log.votes,3), data = movies)
summary(lm.log.votes.3)
```

The first graph is produced by fitting a polynomial of degree 3, while the graph on the right is produced by fitting a regular line. It is very clear how the polynomial of degree 3 allows us to fit a more flexible curve that fits the data better. The red lines represent the confidence intervals.

<!-- show the p-values and adjusted r-squared --> 

```{r Votes P3 Details}
par(mfrow = c(1,2))

poly.fit = lm(rating ~ poly(log.votes, 3), data = movies)
lm.votes.fit = lm(rating ~ log.votes, data = movies)
vote.lims = range(movies$log.votes)
vote.grid = seq(from=vote.lims[1], to=vote.lims[2])

predict.linear = predict(lm.votes.fit, newdata = list(log.votes = vote.grid), se = T)
lin.bands = cbind(predict.linear$fit + 2*predict.linear$se,  predict.linear$fit - 2*predict.linear$se)
predict.poly = predict(poly.fit, newdata = list(log.votes = vote.grid), se = T)
se.bands = cbind(predict.poly$fit + 2*predict.poly$se,  predict.poly$fit - 2*predict.poly$se)

plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Polynomial of d = 3')
lines(vote.grid, predict.poly$fit, lwd = 2, col = 'darkblue')
matlines(vote.grid, se.bands, lwd = 2, col = 'red')
plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Linear Regression')
lines(vote.grid, predict.linear$fit, lwd = 2, col = 'darkblue')
matlines(vote.grid, lin.bands, lwd = 2, col = 'red')
```

We compute the test error using as predictor a polynomial of degree three of votes. We obtain a test error of 1.4607, which is good result considering we are only using one predictor.

```{r Votes P3 Error}
poly.train = lm(rating ~ poly(log.votes, 3), data = train.set)
poly.test = predict(poly.train, newdata = list(log.votes = test.set$log.votes))
poly.error = mean((poly.test - test.set$rating)^2)
print("MSE Test Error:")
poly.error
```

Next we fit a cubic spline to the data. We start by using cross-validation to obtain the degrees of freedom which minimize the test error. We find that 7 degrees of freedom provide us the lowest estimated test error.

```{r Cross Validation}
library(boot)
cross.spline = rep(NA, 12)
for (i in 5:16) {
  fiting = glm(rating ~ bs(log.votes, df = i), data = movies)
  cross.spline[i] = cv.glm(movies, fiting, K = 10)$delta[2]
}
spline.best.df = which.min(cross.spline)
```

<!-- needs proper phrasing -->
Fitting a 7-degree polynomial is equivalent to adding 4 knots to the model. We determine the points where we will fit the 4 knots.
```{r Knots}
attr(bs(log(movies$votes) ,df = spline.best.df) ,"knots") 
```

Next we fit a spline to the data. We can observe that the cubic spline with 4 knots has additional flexibility compared to the polynomial regression because we are fitting four different polynomial regression.

<!--side by side plots (or maybe overlap on same plots) -->

```{r Splines}
library(splines)
spline.fit = lm(rating ~ bs(log.votes, df = spline.best.df), data = movies)
spline.predict = predict(spline.fit, newdata = list(log.votes = vote.grid))
summary(spline.fit)

plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Cubic Sline with 4 Knots')
lines(vote.grid, spline.predict, col = 'darkgreen', lwd = 2)
abline(v = c(attr(bs(movies$log.votes ,df=7) ,"knots") ), col = 'darkorange', lty = 2, lwd = 2)
plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Polynomial of d = 3')
lines(vote.grid, predict.poly$fit, lwd = 2, col = 'darkblue')

```

We obtain a test error, 1.464, that is slightly higher than the polynomial regression. It looks likes the additional flexibility could be increasing the error due to higher variance.

```{r Splines Error}
spline.train = lm(rating ~ bs(log.votes, df = spline.best.df), data = train.set)
spline.test = predict(spline.train, newdata = list(log.votes = test.set$log.votes))
spline.error = mean((spline.test - test.set$rating)^2)
print("MSE Test Error:")
spline.error
```

Next we try fitting a natural spline. We start by using cross-validation to obtain the degrees of freedom which minimize the test error estimate.

<!-- add a graph with cross-validation -->

```{r Splines Validation}
cross.spline = rep(NA, 12)
for (i in 5:16) {
  fiting = glm(rating ~ ns(log.votes, df = i), data = movies)
  cross.spline[i] = cv.glm(movies, fiting, K = 10)$delta[2]
}
ns.best.df = which.min(cross.spline)
```

We fit a natural cubic splin with 6 knots. We observe the same adjusted R squared than what we obtained from the cubic spline. In addition, we graph both models and observe how the natural cubic spline has a little more flexibility due to the higher number of knots. However, we do not observe any improved performance. Particularly because in the boundaries there is still a large number of observations. As a result, the cubic spline is as stable as the natural cubic spline.

```{r Cubic Spline}
par(mfrow = c(1,1))
ns.fit = lm(rating ~ ns(log.votes, df = ns.best.df), data = movies)
ns.predict = predict(ns.fit, newdata = list(log.votes = vote.grid), se = T)
summary(ns.fit)

plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Natural Spline with 6 Knots')
lines(vote.grid, ns.predict$fit, col = 'darkgreen', lwd = 2)
abline(v = c(attr(ns(movies$log.votes ,df=7) ,"knots")) , col = 'darkorange', lty = 2, lwd = 2)
lines(vote.grid, spline.predict, col = 'darkblue', lwd = 2)
legend(9,4, c('Nat. Cubic Spline', 'Cubic Spline'), col = c('darkgreen', 'darkblue'), lty = c(1,1), lwd = c(2,2))
```

We compute the test error and obtain 1.462. This is almost the same from what we obtained using the cubic spline. The conclusion is that the cubic spline is as stable as the natural cubic spline in the boundaries. In addition, the larger number of knots being used by the natural cubic spline is not providing any additional improvement.

<!-- less output here -->

```{r Cubic Splines Details}
ns.train = lm(rating ~ ns(log.votes, df = ns.best.df), data = train.set)
ns.test = predict(ns.train, newdata = list(log.votes = test.set$log.votes))
ns.error = mean((ns.test - test.set$rating)^2)
ns.error
```

We now fit a smoothing spline.

<!-- add confidence interval -->
```{r Smoothing Spline}
smoothing.fit = smooth.spline(movies$log.votes, movies$rating, df = 9)
plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Smoothing Spline')
lines(smoothing.fit, col = 'red', lwd = 2)
#summary(smoothing.fit)
```

We compute the test error and obtain 1.457. This model is obtaining a slightly better performance compared to the cubic and natural spline, but not compared to the polynomial regression with the best subset predictors.

```{r Smoothing Spline Error}
smoothing.train = smooth.spline(x = train.set$log.votes, y = train.set$rating, df = 9)
smoothing.test = predict(smoothing.train, x = test.set$log.votes)$y
smoothing.error = mean((smoothing.test - test.set$rating)^2)
smoothing.error
```


```{r XXX}
# local.errors = rep(NA, 101)
# seq = seq(0.01, 1, by = 0.01)
# j = 1
# for(i in seq){
#   local.train = loess(rating ~ log.votes, span = i , data = train.set)
#   local.test = predict(local.fit, newdata = data.frame(log.votes = test.set$log.votes))$fit
#   errors = mean((local.test - test.set$log.votes)^2)
#   local.errors[j] = errors
#   j = j + 1
# }
# 
# local.fit = loess(rating ~ log.votes, span = 0.05, data = movies)
# local.predict = predict(local.fit, newdata = data.frame(log.votes = vote.grid))
# 
# plot(movies$log.votes, movies$rating, col = 'darkgrey', xlab = 'Votes', ylab = 'Rating', main = 'Votes vs Rating')
# lines(vote.grid, local.predict, col = 'red', lwd = 2)
```

[^1]: Check code annex for details.

### General Additive Models (GAMs)
We will now explore General Additive Models (GAMs). These extend the multiple linear regression model by allowing non-linear functions of each variable. We calculate a different function for each variable, and add together all of their contributions. This results in potentially more accurate predictions for our response variable, the movie rating. Furthermore, since the model is additive, we can interpret the effects of each variable on the movie rating.    

#### GAM with Smoothing Splines
We use the predictors that were chosen in the best subset selection: year, length, votes, Animation, Comedy, Drama, and adjBudget. We use the 'gam' package in R to fit the models. This software relies on the backfitting method to fit the model.
```{r}
fit.1 = gam(rating ~ year + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.2 = gam(rating ~ s(year, df = 4) + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.3 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.4 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(votes, df = 4) + Animation + Comedy + Drama + adjBudget, data = movies)
fit.5 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(votes, df = 4) + Animation + Comedy + Drama + s(adjBudget, df = 4), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```
Based on the ANOVA results, there is evidence to reject the null hypothesis that the simplest model is sufficient. Thus, the GAM with a smoothing spline for each of year, length, and budget, along with linear variables animation, comedy, and drama, is the best model.

We now make predictions on the training set using the best GAM and compute the test MSE.
```{r}
gam.train = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(votes, df = 4) + Animation + Comedy + Drama + s(adjBudget, df = 4), data = train.set)
gam.test = predict(gam.train, newdata = test.set)
gam.error = mean((gam.test - test.set$rating)^2)
gam.error
```
The test MSE is 0.944.

#### GAM with Natural Splines
We repeat the process of selecting the best GAM, this time using natural splines instead of smoothing splines. Again, we use the same variables from the best subset selection.
```{r}
fit.1 = gam(rating ~ year + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.2 = gam(rating ~ ns(year, df = 4) + length + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.3 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.4 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(votes, df = 4) + Animation + Comedy + Drama + adjBudget, data = movies)
fit.5 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(votes, df = 4) + Animation + Comedy + Drama + ns(adjBudget, df = 4), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

```{r}
gam.train = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(votes, df = 4) + Animation + Comedy + Drama + ns(adjBudget, df = 4), data = train.set)
gam.test = predict(gam.train, newdata = test.set)
gam.error = mean((gam.test - test.set$rating)^2)
gam.error
```

The natural splines GAM yields a Test MSE of 0.974.

Thus, the best GAM consists of smoothing splines instead of natural splines. With a test MSE of 0.944, this is the most accurate model at this point in our analysis. 

However, we found in our modeling with polynomial regression and splines that taking the logarithm of the votes improves the accuracy of the model. Is this true for the GAM as well? We repeat the analysis above, substituting the logarithm of the votes for the untransformed votes.

#### GAM with Smoothing Splines and Logarithm of Votes
```{r}
fit.1 = gam(rating ~ year + length + log.votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.2 = gam(rating ~ s(year, df = 4) + length + log.votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.3 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + log.votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.4 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(log.votes, df = 4) + Animation + Comedy + Drama + adjBudget, data = movies)
fit.5 = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(log.votes, df = 4) + Animation + Comedy + Drama + s(adjBudget, df = 4), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

```{r}
gam.train = gam(rating ~ s(year, df = 4) + s(length, df = 4) + s(log.votes, df = 4) + Animation + Comedy + Drama + s(adjBudget, df = 4), data = train.set)
gam.test = predict(gam.train, newdata = test.set)
gam.error = mean((gam.test - test.set$rating)^2)
gam.error
```

Based on the results from ANOVA, the best GAM that uses smoothing splines and the logarithmic votes is the most complex, with a spline for year, length, log(votes), and adjBudget. It has a test MSE of 0.937. This is slightly better than the test MSE for the identical model with normal votes instead of log(votes), which had a test MSE of 0.944.

```{r}
par(mfrow = c(1,7))
plot(fit.5, se=TRUE, col="red")
```

We display a plot for each variable in our GAM, showing its individual contribution to the additive model. The standard errors are repressented by the dotted lines.

Plots of the relationship between each feature and the response, movie ratings, in the fitted GAM. Each plot displays the fitted function and pointwise standard errors. The first three functions are smoothing splines in year, length, and logarithm votes, with four degrees of freedom. The fourth, fifth, and sixth plots display the qualitative variables Animation, Comedy, and Drama, respectively. The rightmost function is the smoothing spline in budget. 

If we hold all variables constant except for the year, the leftmost plot tells us that the rating decreases with the year. The second plot tells us that holding all variables fixed except for length, the movie rating increases until approximately 250 minutes, and then begins to decrease. Other interpretations:
* Holding all of our predictors fixed except for votes, the movie rating increases with the number of votes.
* Holding all factors fixed except for budget, as the budget increases, the movie rating decreases. However, note the larger standard errors here.
* Animation, Comedy, and Drama each contributes to an increase in movie rating, which supports our correlation analysis

#### GAM with Natural Splines and Logarithm of Votes
```{r}
fit.1 = gam(rating ~ year + length + log.votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.2 = gam(rating ~ ns(year, df = 4) + length + log.votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.3 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + log.votes + Animation + Comedy + Drama + adjBudget, data = movies)
fit.4 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(log.votes, df = 4) + Animation + Comedy + Drama + adjBudget, data = movies)
fit.5 = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(log.votes, df = 4) + Animation + Comedy + Drama + ns(adjBudget, df = 4), data = movies)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

```{r}
gam.train = gam(rating ~ ns(year, df = 4) + ns(length, df = 4) + ns(log.votes, df = 4) + Animation + Comedy + Drama + ns(adjBudget, df = 4), data = train.set)
gam.test = predict(gam.train, newdata = test.set)
gam.error = mean((gam.test - test.set$rating)^2)
gam.error
```

The best GAM that uses natural splines and logarithmic votes has a test MSE of 0.980, making it the least accurate of the GAMs.

We conclude that the GAM with smoothing splines is the best model for predicting the rating of a movie. Furthermore, taking the logarithm of the votes provides a slight improvement to the accuracy of the model; however, the benefit of the logarithm was more substantial in our linear models.

# Conclusion

<!-- sort this, add correct values -->
------------------------------------------------------------
Model                     Test MSE
------------------------- ----------------------------------
Linear Regression           0.981
- using subset selection
- with log(votes)

Polynomial Regression       8.334
- with votes       

Polynomial Regresssion      0.937
- using subset selection
 
Spline                      4.449

GAM                         0.944
- smoothing splines         

GAM                         0.974
- natural splines

GAM
- smoothing splines         0.937
- log(votes)

GAM
- natural splines           0.980
- log(votes)
------------------------------------------------------------
